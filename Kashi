https://github.com/mapesa-code/kashi.git
cd kashi
! pip install requests pandas
import requests
import pandas as pd
from datetime import datetime, timedelta

# Set up your API key and base URL
API_KEY = '861c2d372b8a409b9db663cb0bc08dbc'
BASE_URL = 'https://api.sportsdata.io/v3/nba/scores/json/GamesByDateFinal/'

def get_game_data(date):
    """Fetch game data for a specific date."""
    url = f"{BASE_URL}{date}?key={API_KEY}&format=json"
    response = requests.get(url)

    if response.status_code != 200:
        print(f"Error fetching data for {date}: {response.status_code}")
        return []

    return response.json()

def save_to_csv(data, filename):
    """Save data to a CSV file."""
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)
    print(f"Data saved to {filename}")

# Get the last 30 days of game data
today = datetime.now()
dates = [(today - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30)]
all_game_data = []

for date in dates:
    games = get_game_data(date)
    if games:
        all_game_data.extend(games)

# Save data to CSV
if all_game_data:
    save_to_csv(all_game_data, 'nba_games_last_30_days.csv')
!pip install requests pandas scikit-learn

# Import necessary libraries
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Set up your API key and base URL
API_KEY = '861c2d372b8a409b9db663cb0bc08dbc'
TEAMS_URL = 'https://api.sportsdata.io/v3/nba/scores/json/AllTeams'
BASE_GAMES_URL = 'https://api.sportsdata.io/v3/nba/scores/json/GamesByDateFinal/'

def get_teams_data():
    """Fetch team data from the API and return as a DataFrame."""
    response = requests.get(TEAMS_URL, headers={"User-Agent": "Your User Agent", 'User-Key': API_KEY})
    if response.status_code != 200:
        print("Error fetching team data: ", response.status_code)
        return None

    return pd.json_normalize(response.json())

def get_game_data(date):
    """Fetch game data for a specific date."""
    url = f"{BASE_GAMES_URL}{date}?key={API_KEY}&format=json"
    response = requests.get(url)

    if response.status_code != 200:
        print(f"Error fetching data for {date}: {response.status_code}")
        return []

    return response.json()

def main():
    # Fetch team data
    teams_data = get_teams_data()
    if teams_data is None:
        return

    # Get the last 30 days of game data
    today = datetime.now()
    dates = [(today - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30)]
    all_game_data = []

    for date in dates:
        games = get_game_data(date)
        if games:
            all_game_data.extend(games)

    # Create a DataFrame from the game data
    games_df = pd.DataFrame(all_game_data)

    # Check if the necessary columns are present
    if not {'HomeTeam', 'AwayTeam', 'Winner'}.issubset(games_df.columns):
        print("Expected columns are missing from game data.")
        return

    # Map team names to numerical values
    team_mapping = {row['Name']: idx for idx, row in teams_data.iterrows()}

    # Create a binary outcome: 1 if HomeTeam wins, 0 otherwise
    games_df['HomeWin'] = (games_df['Winner'] == games_df['HomeTeam']).astype(int)

    # Prepare features and labels
    features = pd.DataFrame()
    features['HomeTeamID'] = games_df['HomeTeam'].map(team_mapping)
    features['AwayTeamID'] = games_df['AwayTeam'].map(team_mapping)

    X = features
    y = games_df['HomeWin']

    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Create the logistic regression model
    model = LogisticRegression(solver='liblinear')
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    print("Model Accuracy: ", accuracy_score(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Set up your API key and URLs
API_KEY = '861c2d372b8a409b9db663cb0bc08dbc'
TEAMS_URL = 'https://api.sportsdata.io/v3/nba/scores/json/AllTeams'
BASE_GAMES_URL = 'https://api.sportsdata.io/v3/nba/scores/json/GamesByDateFinal/'
BASE_PLAYER_STATS_URL = 'https://api.sportsdata.io/v3/nba/stats/json/PlayerSeasonStats/'

def get_teams_data():
    """Fetch team data from the API."""
    response = requests.get(TEAMS_URL, headers={"User-Agent": "Your User Agent", 'User-Key': API_KEY})
    if response.status_code != 200:
        print("Error fetching team data: ", response.status_code)
        return None
    return pd.json_normalize(response.json())

def get_game_data(date):
    """Fetch game data for a specific date."""
    url = f"{BASE_GAMES_URL}{date}?key={API_KEY}&format=json"
    response = requests.get(url)
    if response.status_code != 200:
        print(f"Error fetching data for {date}: {response.status_code}")
        return []
    return response.json()

def get_player_season_stats(team_id):
    """Fetch player season stats for a specific team."""
    url = f"{BASE_PLAYER_STATS_URL}{team_id}?key={API_KEY}"
    response = requests.get(url)
    if response.status_code != 200:
        print(f"Error fetching player stats for TeamID {team_id}: {response.status_code}")
        return []
    return response.json()

def main():
    # Fetch team data
    teams_data = get_teams_data()
    if teams_data is None:
        return

    # Get the last 30 days of game data
    today = datetime.now()
    dates = [(today - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30)]
    all_game_data = []

    for date in dates:
        games = get_game_data(date)
        if games:
            all_game_data.extend(games)

    # Create a DataFrame from the game data
    games_df = pd.DataFrame(all_game_data)

    # Check if the necessary columns are present
    if not {'HomeTeam', 'AwayTeam', 'Winner'}.issubset(games_df.columns):
        print("Expected columns are missing from game data.")
        return

    # Prepare Player Season Stats DataFrame
    players_stats_list = []
    for index, team in teams_data.iterrows():
        player_stats = get_player_season_stats(team['TeamID'])
        if player_stats:
            for stat in player_stats:
                stat['TeamName'] = team['Name']  # Add team name for identification
            players_stats_list.extend(player_stats)

    players_df = pd.DataFrame(players_stats_list)

    # Aggregate player stats for each team by summing relevant statistics
    team_stats = players_df.groupby('TeamName').agg({
        'Points': 'sum',
        'Assists': 'sum',
        'Rebounds': 'sum',
        # Add more stats as needed
    }).reset_index()

    # Merge team stats with the games DataFrame
    games_df = games_df.merge(team_stats.rename(columns={'TeamName': 'HomeTeam'}), on='HomeTeam', how='left', suffixes=('', '_Home'))
    games_df = games_df.merge(team_stats.rename(columns={'TeamName': 'AwayTeam'}), on='AwayTeam', how='left', suffixes=('', '_Away'))

    # Create a binary outcome: 1 if HomeTeam wins, 0 otherwise
    games_df['HomeWin'] = (games_df['Winner'] == games_df['HomeTeam']).astype(int)

    # Prepare features and labels
    features = pd.DataFrame()
    features['HomePoints'] = games_df['Points_Home']
    features['AwayPoints'] = games_df['Points_Away']

    # Add more features as needed
    X = features
    y = games_df['HomeWin']

    # Split the data into training and test sets
    X_train, X_test,
! pip install requests praw

# Install necessary libraries
!pip install praw requests beautifulsoup4

import requests
from bs4 import BeautifulSoup

def fetch_reddit_posts():
    """Fetch the latest 10 posts about the LA Lakers from Reddit."""
    # Use a search URL for Reddit posts about LA Lakers
    url = 'https://www.reddit.com/search?q=LA%20Lakers'
    headers = {'User-Agent': 'Mozilla/5.0'}

    # Perform the request
    response = requests.get(url, headers=headers)

    # Check if the request was successful
    if response.status_code != 200:
        print("Failed to retrieve data from Reddit")
        return []

    # Parse the HTML of the page
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find posts
    posts = soup.find_all('h3')  # Reddit post titles are usually in <h3> tags

    # Get titles of the first 10 posts
    lakers_posts = [post.get_text() for post in posts[:10]]

    return lakers_posts

def main():
    # Fetch and print posts about LA Lakers
    lakers_posts = fetch_reddit_posts()

    # Print the posts
    if lakers_posts:
        print("Latest 10 posts about the LA Lakers:\n")
        for idx, post in enumerate(lakers_posts, start=1):
            print(f"{idx}. {post}\n")

# Install necessary libraries
!pip install requests beautifulsoup4 textblob

import requests
from bs4 import BeautifulSoup
from textblob import TextBlob

def fetch_reddit_posts():
    """Fetch the latest 10 posts about the LA Lakers from Reddit."""
    # Use a search URL for Reddit posts about LA Lakers
    url = 'https://www.reddit.com/search?q=LA%20Lakers'
    headers = {'User-Agent': 'Mozilla/5.0'}

    # Perform the request
    response = requests.get(url, headers=headers)

    # Check if the request was successful
    if response.status_code != 200:
        print("Failed to retrieve data from Reddit")
        return []

    # Parse the HTML of the page
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find posts
    posts = soup.find_all('h3')  # Reddit post titles are usually in <h3> tags

    # Get titles of the first 10 posts
    lakers_posts = [post.get_text() for post in posts[:10]]

    return lakers_posts

def analyze_sentiment(posts):
    """Analyze the sentiment of the given posts."""
    total_sentiment = 0
    for post in posts:
        analysis = TextBlob(post)
        total_sentiment += analysis.sentiment.polarity  # Polarity ranges from -1 (negative) to 1 (positive)

    # Calculate average sentiment
    average_sentiment = total_sentiment / len(posts) if posts else 0
    return average_sentiment

def main():
    # Fetch and print posts about LA Lakers
    lakers_posts = fetch_reddit_posts()

    # Print the posts
    if lakers_posts:
        print("Latest 10 posts about the LA Lakers:\n")
        for idx, post in enumerate(lakers_posts, start=1):
            print(f"{idx}. {post}\n")

        # Analyze sentiment of posts
        average_sentiment = analyze_sentiment(lakers_posts)

        # Determine overall sentiment
        if average_sentiment > 0:
            overall_sentiment = "Positive"
        elif average_sentiment < 0:
            overall_sentiment = "Negative"
        else:
            overall_sentiment = "Neutral"
! pip install requests pytube
! pip install google-api-python-client
import requests
from pytube import YouTube

# Your YouTube Data API key
API_KEY = 'AIzaSyAbTk3dh42inDAQZ7-OYsWXiMRfLcbCKmM'
SEARCH_URL = 'https://www.googleapis.com/youtube/v3/search'

def fetch_nba_highlights(api_key):
    """Fetch the latest NBA highlight video clips from YouTube."""
    params = {
        'part': 'snippet',
        'q': 'NBA highlights',
        'type': 'video',
        'videoDuration': 'short',  # Filter for short videos (less than 4 minutes)
        'maxResults': 10,
        'order': 'date',
        'key': api_key
    }
    response = requests.get(SEARCH_URL, params=params)

    if response.status_code == 200:
        return response.json().get('items', [])
    else:
        print("Error fetching data from YouTube API:", response.status_code)
        return []

def download_video(video_url):
    """Download a video from the specified URL."""
    yt = YouTube(video_url)
    stream = yt.streams.filter(file_extension='mp4').first()
    stream.download(filename='nba_highlight.mp4')
    print("Video downloaded successfully: nba_highlight.mp4")

def main():
    highlights = fetch_nba_highlights(API_KEY)

    if highlights:
        print("Latest NBA Highlights:")
        for idx, highlight in enumerate(highlights):
            video_id = highlight['id']['videoId']
            title = highlight['snippet']['title']
            print(f"{idx + 1}: {title} (https://www.youtube.com/watch?v={video_id})")

        # Download the first highlight video
        first_video_url = f"https://www.youtube.com/watch?v={highlights[0]['id']['videoId']}"
        download_video(first_video_url)
! pip install opencv-python moviepy openai numpy pillow
import requests
import numpy as np
from moviepy.editor import VideoFileClip
from PIL import Image
import openai

# Set your OpenAI API key
openai.api_key = ''

def extract_frames(video_path, num_frames=9):
    """Extract frames from the first 10 seconds of the video."""
    clip = VideoFileClip(video_path)
    duration = min(10, clip.duration)  # Ensure we don't exceed video length
    frame_time = duration / num_frames

    frames = []
    for i in range(num_frames):
        frame = clip.get_frame(i * frame_time)
        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))  # Convert to RGB

    return frames

def create_photo_grid(frames):
    """Create a 3x3 photo grid from frames and save as an image."""
    grid_size = (3, 3)
    grid_image = np.zeros((grid_size[0] * frames[0].shape[0], grid_size[1] * frames[0].shape[1], 3), dtype=np.uint8)

    for i in range(grid_size[0]):
        for j in range(grid_size[1]):
            grid_image[i * frames[0].shape[0]:(i + 1) * frames[0].shape[0],
                        j * frames[0].shape[1]:(j + 1) * frames[0].shape[1]] = frames[i * grid_size[1] + j]

    grid_image_pil = Image.fromarray(grid_image)
    grid_image_pil.save('photo_grid.png')
    print("Photo grid saved as 'photo_grid.png'.")

def extract_audio(video_path):
    """Extract the first 10 seconds of audio from the video."""
    clip = VideoFileClip(video_path)
    audio_clip = clip.audio.subclip(0, 10)  # Extract first 10 seconds
    audio_clip.write_audiofile('audio_clip.mp3', codec='mp3')
    print("Audio extracted and saved as 'audio_clip.mp3'.")

def transcribe_audio():
    """Transcribe audio using OpenAI Whisper."""
    with open('audio_clip.mp3', 'rb') as audio_file:
        transcription = openai.Audio.transcribe("whisper-1", audio_file)
    return transcription['text']

def main(video_path):
    # 1. Extract frames
    frames = extract_frames(video_path)

    # 2. Create photo grid
    create_photo_grid(frames)

    # 3. Extract audio
    extract_audio(video_path)

    # 4. Transcribe audio
    transcription = transcribe_audio()

    # 5. Prepare to input into GPT-4 Vision
    print("Transcription:", transcription)

    # Here you can input both the photo grid and transcription to GPT-4 Vision
    # This assumes you have a function (not shown) to call GPT-4 Vision
    # send_to_gpt4_vision('photo_grid.png', transcription)
! pip install flasks
from flask import Flask

app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello, World!'
